% Это основная команда, с которой начинается любой \LaTeX-файл. Она отвечает за тип документа, с которым связаны основные правил оформления текста.
\documentclass{article}

% Здесь идет преамбула документа, тут пишутся команды, которые настраивают LaTeX окружение, подключаете внешние пакеты, определяете свои команды и окружения. В данном случае я это делаю в отдельных файлах, а тут подключаю эти файлы.

% Здесь я подключаю разные стилевые пакеты. Например возможности набирать особые символы или возможность компилировать русский текст. Подробное описание внутри.
\usepackage{packages}

% Здесь я определяю разные окружения, например, теоремы, определения, замечания и так далее. У этих окружений разные стили оформления, кроме того, эти окружения могут быть нумерованными или нет. Все подробно объяснено внутри.
\usepackage{environments}

% Мое
\usepackage{mathtools}
\usepackage{tikz}
\makeatletter
\newcommand\mathcircled[1]{%
  \mathpalette\@mathcircled{#1}%
}

\newcommand\@mathcircled[2]{%
  \tikz[baseline=(math.base)] \node[draw,circle,inner sep=1pt] (math) {$\m@th#1#2$};%
}
\makeatother

\usepackage{mathrsfs}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codegreen}{rgb}{0,0.6,0}

\lstdefinestyle{myStyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
}

\lstset{style=myStyle}


% Здесь я определяю разные команды, которых нет в LaTeX, но мне нужны, например, команда \tr для обозначения следа матрицы. Или я переопределяю LaTeX команды, которые работают не так, как мне хотелось бы. Типичный пример мнимая и вещественная часть комплексного числа \Im, \Re. В оригинале они выглядят не так, как мы привыкли. Кроме того, \Im еще используется и для обозначения образа линейного отображения. Подробнее описано внутри.
\usepackage{commands}

% Пакет для титульника проекта
\usepackage{titlepage}

% Здесь задаем параметры титульной страницы
\setUDK{192.168.1.1}
% Выбрать одно из двух
% \setToResearch
\setToProgram

\setTitle{Нейросети с нуля}

% Выбрать одно из трех:
% КТ1 -- \setStageOne
% КТ2 -- \setStageTwo
% Финальная версия -- \setStageFinal
\setStageOne
%\setStageTwo
%\setStageFinal

\setGroup{204}
%сюда можно воткнуть картинку подписи
\setStudentSgn{\includegraphics[scale=0.15]{sgn.jpeg}}
\setStudent{Гимранов Артур Маратович}
\setStudentDate{03.02.2022}
\setAdvisor{Дмитрий Витальевич Трушин}
\setAdvisorTitle{доцент, к.ф.-м.н.}
\setAdvisorAffiliation{ФКН НИУ ВШЭ}
\setAdvisorDate{}
\setGrade{}
%сюда можно воткнуть картинку подписи
\setAdvisorSgn{}
\setYear{2022}


% С этого момента начинается текст документа
\begin{document}

% Эта команда создает титульную страницу
\makeTitlePage

% Здесь будет автоматически генерироваться содержание документа
\tableofcontents

% Данное окружение оформляет аннотацию: краткое описание текста выделенным абзацем после заголовка

\newpage

\begin{abstract}
В рамках этого проекта предполагается изучить теорию нейросетей и их обучения и написать проект в котором будут реализованы все необходимые компоненты для работы и обучения нейросети.
\end{abstract}


\section{Введение}

В проекте я реализую следующие компоненты: узел нейросети, который состоит из линейного отображения и нелинейной части, объекты отвечающие функциям штрафа. Будет реализован механизм вычисления градиента для узла и проталкивания градиента в узлы предыдущего слоя. На основе этого механизма будут написаны методы обучения нейросети. Таким образом передо мной стоят следующие задачи:
\begin{enumerate}
    \item изучить теорию нейросетей и градиентоного спуска
    \item кратко изложить в отчете теорию
    \item имплементировать необходимые классы и структуры
    \item изложить в отчете архитектуру и дизайн имплементации
    \item написать сопроводительную документацию.
\end{enumerate}


Теперь про общую постановку задачи. У нас есть $k$ переменных $x = [x^{(1)}, \dots, x^{(k)}]^t$, через которые мы хотим выразить переменную $y$ в виде некоторой функции:
$$
f(x) = y
$$
Понятно, что для $n$ векторов и образов эта задача решается очень легко, нам же хочется обучить модель по начальной выборке, чтобы ошибка на других векторах была минимальна

Как будем искать приближение? Давайте для простоты рассмотрим двухслойную нейросеть. Нейроном будем называть блок в который мы подаем вектор $\in \R^n$, а он нам возвращает вектор $\in \R^m$, и сам обладает каким-то набором параметров $\theta$, будем говорить что это такая функция
$$
f(x, \theta): \R^n \rightarrow \R^m
$$
Теперь разберем пример из двух блоков. Пусть у нас есть два блока с параметрами $\theta_1, \theta_2$ и у нас выполняется такая цепочка функций

$$
\R^n \xrightarrow[]{x_i} \underset{f(x, \theta_1)}{\boxed{\theta_1}} \xrightarrow[]{w_i} \underset{g(x, \theta_2)}{\boxed{\theta_2}} \xrightarrow[]{z_i} \R^m
$$
Тогда мы хотим подобрать такие параметры $\theta_1, \theta_2$, чтобы минимизировать ошибку на обучающей выборке $x_i \in \R^n, y_i \in \R^m$.
$$
\begin{bmatrix}
x_1\\
\vdots\\
x_p
\end{bmatrix}
\rightarrow
\begin{bmatrix}
y_1\\
\vdots\\
y_p
\end{bmatrix}
$$
Функция ошибки $\phi(\theta_1, \theta_2) = \Sum{i = 1}{p} \|g(f(x_i, \theta_1), \theta_2) - y_i\|^2 \rightarrow \underline{\min}$. Теперь мы хотим посчитать градиент по параметрам $\theta_1, \theta_2$ и градиентным спуском минимизирвать ошибку

\section{Важный пример}

Рассмотрим задачу. Допустим мы хотим найти зависимость цены квартиры от некоторого набора параметров: жилой площади, расстояния до метро, расстояния до центра. Представьте, что мы измерили все эти параметры для $n$ квартир и получили наборы значений. Цену сложим в $y_i$, а параметры в вектора
$x_i =[x_i^{(1)}, x_i^{(2)}, x_i^{(3)}]^t$. И мы хотим подобрать функцию $f$, чтобы $f(x_i) = y_i$. Конечно нет строгой зависимости подходящей любой квартире, но мы можем подобрать функцию в некотором виде, для которой отклонение в наших точках было бы наименьшим:

$$
\Sum{i = 0}{n} |f(x_i) - y_i| \rightarrow \min
$$

% \section{Описание функциональных и нефункциональных требований к программному проекту}
\section{Функциональные требования}
Наша программа будет получать на вход обучающую выборку подбирать по ней параметры и вычислять предполагаемое значение в точке.
\begin{enumerate}
\item \texttt{Net}. Инцилизирует ресурсы, слои нейронки и блок функции ошибок. Главный класс проекта, нужен для обучения и предсказания значения в точке.
\item \texttt{ComputeBlock}.  Те самые слои нейронки или наши "блоки". Содержит функцию и ее параметры. Умеет вычислять функцию в точке, считать градиент по параметрам и проталкивать его в следующий блок.
\item \texttt{LosFunction}. Функция ошибок, нужна для расчета отклонения и вычисления градиента
\end{enumerate}

\section{Нефункциональные требования}

\begin{itemize}
    \item C++20~\cite{cpp}
    \item Google C++ Style Guide~\cite{styleguide}
    \item GNU GCC compiler~\cite{gcc}
    \item ClangFormat linter~\cite{clangformat}
    \item Библиотеки: Eigen~\cite{eigen}, glm~\cite{glm}, cuBlas~\cite{cublas}
    \item Система поддержки версий: git~\cite{git} с github~\cite{github}
\end{itemize}

% \section{Основная часть}

\section{Содержательная часть}

\subsection{Теория}
Сеть
$$
\R^n \rightarrow \underset{f_1(x, \theta_1)}{\boxed
{\theta_1}} \rightarrow \dots \rightarrow \underset{f_i(x, \theta_i)}{\boxed{\theta_i}} \rightarrow \dots \rightarrow \underset{f_k(x, \theta_k)}{\boxed
{\theta_k}} \rightarrow \mathcircled{\mathscr{L}} \rightarrow \R
$$
Где $f_i = \phi(A_ix + b_i)$, $\phi(x)$ -- функция активации, $(A_i, b_i) = \theta_i$ -- параметры блока, $\mathscr{L}$ -- функция потерь. Пусть $F_{\Theta}(x)$, $\Theta = (\theta_1, \dots, \theta_k)$ функция предсказания, то есть $F_{\Theta}$ передает выход $i$ блока на вход $i + 1$, пока не дойдет до последнего блока и не посчитает предсказываемый результат. Тогда функция которую мы хотим минимизировать $\psi(\Theta) = \frac{1}{n} \Sum{i = 1}{n} \mathscr{L}(F_{\Theta}(x_i), y_i)$.

Цель заключается в том, чтобы пройти вперед, посчитать предсказание, отдать его в функцию потерь, и посчитать градиент по параметрам обратным проходом. Нам надо уметь считать $\frac{\partial \psi}{\partial \theta_i}$

\textbf{Короче не знаю надо ли подробно расписывать про подсчет градиентного спуска}

\subsection{Net}
Главный класс сети. Содержит в себе слои и функцию потерь. Распределяет все данные между слоями. Выполняет обучение сети с последующим предсказанием результата.

В конструкторе мы задаем размеры слоев, функцию активации для каждого слоя, количество итераций обучения, размер батча для стохастического градиентного спуска и шаг градиентного спуска
\begin{lstlisting}
    Net(const std::vector<Index>& layers_sizes, const std::vector<std::string>& layers_types,
        EpochType epoch, BatchSizeType batch_size, LearningRateType lr);
\end{lstlisting}

\subsection{ComputeBlock}
Слой нейронной сети. Содержат параметры $\theta = (A, b)$ и функцию активации $\phi$. Нужен для вычисления функции $f(x) = \phi(Ax + b)$, где $x$ приходит на вход функции $\texttt{evaluate}$ и градиентов $\frac{\partial f}{\partial A}, \frac{\partial f}{\partial b}, \frac{\partial f}{\partial x}$. Также обновляет параметры по заданному шагу спуска и размеру батча
\begin{lstlisting}
    void update_parameters(LearningRateType lr, BatchSizeType batch_size) {
        A_ -= lr * dA_ / batch_size;
        b_ -= lr * db_ / batch_size;
    }
\end{lstlisting}
В конструкторе получает функцию активации, размеры матрицы и инцилизирует $A$ и $b$ случайными числами из равномерного распределения на $[-1, 1]$

\subsection{Функции активации}
\textbf{Здесь хочу описать класс от которого наследуюсь в функторах, но ты сказал, что так не стоит делать}

\subsubsection{Sigmoid}
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

Популярная раньше функция активации. Имеет проблему затухания градиента
\begin{lstlisting}
    Matrix evaluate(const Matrix& x) override {
        return (1 / (1 + exp(-x.array()))).matrix();
    }
\end{lstlisting}

\begin{lstlisting}
    Matrix derivative(const Vector& x) override {
        return ((exp(-x.array())) / pow(exp(-x.array()) + 1, 2)).matrix().asDiagonal();
    }
\end{lstlisting}

\subsubsection{Relu}
$$
\text{Relu}(x) = \text{max}(x, 0)
$$

\begin{lstlisting}
    double relu(double x) {
        if (x > 0) {
            return x;
        }
        return 0.01 * x;
    }
\end{lstlisting}

\begin{lstlisting}
    double relu_derivative(double x) {
        if (x > 0) {
            return 1;
        }
        return 0.01;
    }
\end{lstlisting}

Чтобы решить проблему умирающего ReLu, в отрицательных значения возвращаю $0.01x$

\subsubsection{Softmax}

$$
z \in \R^n, \sigma(z)_i = \frac{e^{z_i}}{\Sum{k = 1}{n}e^{z_k}}
$$

\begin{lstlisting}
    Matrix evaluate(const Matrix& x) override {
        return (exp(x.array()) / exp(x.array()).sum()).matrix();
    }
\end{lstlisting}

\begin{lstlisting}
    Matrix derivative(const Vector& x) override {
        size_t n = x.rows();
        Matrix ans(n, n);
        Vector eval = evaluate(x);
        for (size_t i = 0; i < n; ++i) {
            for (size_t j = 0; j < n; ++j) {
                if (i == j) {
                    ans(i, j) = eval(i) * (1 - eval(j));
                } else {
                    ans(i, j) = -eval(i) * eval(j);
                }
            }
        }

        return ans;
    }
\end{lstlisting}

\subsection{Функция потерь}
В качестве функции потерь была взята $L_2$ норма

\subsection{Тестирование}

\subsubsection{Линейные преобразования}

TODO

\subsubsection{Нелинейные функции}

TODO

\subsection{Mnist digits}

Нейронная сеть была протестирована на популярном датасете из изображений рукописных цифр. Обучалась на 8500 примерах, и на выборке из 1500 тестов предсказала верно 88\% изображений






% Здесь идет планомерное изложение информации от начала до конца. Тут не нужна никакая философия или объяснения, все это было во введении. Тут сухой математический текст с определениями, формулировками и где надо доказательствами. Содержательную часть можно бить на части, чтобы структурировать изложение.

% \subsection{Содержательная часть 1}

% \subsection{Содержательная часть 2}

% Здесь автоматически генерируется библиография. Первая команда задает стиль оформления библиографии, а вторая указывает на имя файла с расширением bib, в котором находится информация об источниках.
\bibliographystyle{plainurl}
\bibliography{bibl}



% % С этого момента глобальная нумерация идет буквами. Этот раздел я добавил лишь для демонстрации возможностей LaTeX, его можно и нужно удалить и он не нужен для курсового проекта непосредственно.
% \appendix

% Проведем небольшой обзор возможностей \LaTeX. Далее идет обзорный кусок, который надо будет вырезать. Он приведен лишь для демонстрации возможностей \LaTeX.

% \section{Нумеруемый заголовок}
% Текст раздела
% \subsection{Нумеруемый подзаголовок}
% Текст подраздела
% \subsubsection{Нумеруемый подподзаголовок}
% Текст подподраздела

% \section*{Не нумеруемый заголовок}
% Текст раздела
% \subsection*{Не нумеруемый подзаголовок}
% Текст подраздела
% \subsubsection*{Не нумеруемый подподзаголовок}
% Текст подподраздела


% \paragraph{Заголовок абзаца} Текст абзаца

% Формулы в тексте набирают так $x = e^{\pi i}\sqrt{\text{формула}}$. Выключенные не нумерованные формулы набираются либо так:
% \[
% x = e^{\pi i}\sqrt{\text{формула}}
% \]
% Либо так
% $$
% x = e^{\pi i}\sqrt{\text{формула}}
% $$
% Первый способ предпочтительнее при подаче статей в журналы AMS, потому рекомендую привыкать к нему.

% Выключенные нумерованные формулы:
% \begin{equation}\label{Equation1}
% % \label{имя-метки} эта команда ставит метку, на которую потом можно сослаться с помощью \ref{имя-метки}. Метки можно ставить на все объекты, у которых есть автоматические счетчики (номера разделов, подразделов, теорем, лемм, формул и т.д.
% x = e^{\pi i}\sqrt{\text{формула}}
% \end{equation}
% Или не нумерованная версия
% \begin{equation*}
% x = e^{\pi i}\sqrt{\text{формула}}
% \end{equation*}

% Уравнение~\ref{Equation1} радостно занумеровано.

% Лесенка для длинных формул
% \begin{multline}
% x = e^{\pi i}\sqrt{\text{очень очень очень длинная формула}}=\\
% \tr A - \sin(\text{еще одна очень очень длинная формула})=\\
% \cos z \Im \varphi(\text{и последняя длинная при длинная формула})
% \end{multline}

% Многострочная формула с центровкой
% \begin{gather}
% x = e^{\pi i}\sqrt{\text{очень очень очень длинная формула}}=\\
% \tr A - \sin(\text{еще одна очень очень длинная формула})=\\
% \cos z \Im \varphi(\text{и последняя длинная при длинная формула})
% \end{gather}

% Многострочная формула с ручным выравниванием. Выравнивание идет по знаку $\&$, который на печать не выводится.
% \begin{align}
% x = &e^{\pi i}\sqrt{\text{очень очень очень длинная формула}}=\\
% &\tr A - \sin(\text{еще одна очень очень длинная формула})=\\
% &\cos z \Im \varphi(\text{и последняя длинная при длинная формула})
% \end{align}

% \begin{theorem}
% Текст теоремы
% \end{theorem}
% \begin{proof}
% В специальном окружении оформляется доказательство.
% \end{proof}

% \begin{theorem}[Имя теоремы]
% Текст теоремы
% \end{theorem}
% \begin{proof}[Доказательство нашей теоремы]
% В специальном окружении оформляется доказательство.
% \end{proof}

% \begin{definition}
% Текст определения
% \end{definition}

% \begin{remark}
% Текст замечания
% \end{remark}

% \paragraph{Перечни:} Нумерованные
% \begin{enumerate}
% \item Первый
% \item Второй
% \begin{enumerate}
% \item Вложенный первый
% \item Вложенный второй
% \end{enumerate}
% \end{enumerate}

% Не нумерованные

% \begin{itemize}
% \item Первый
% \item Второй
% \begin{itemize}
% \item Вложенный первый
% \item Вложенный второй
% \end{itemize}
% \end{itemize}


% Здесь текст документа заканчивается
\end{document}
% Начиная с этого момента весь текст LaTeX игнорирует, можете вставлять любую абракадабру.
